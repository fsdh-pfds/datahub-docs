# Office Hours - April 4th, 2023

## Summary

We covered a few topics in the office hour today

- Troubleshooting issues with logging in to the FSDH portal, specifically around the "Not found in Oxygen163" error message.

  > If you encounter this error, please verify that your Workspace Lead has invited you and you have completed the login steps after going to the FSDH. If you are still having issues, please reach out to your Workspace Lead or [submit a support request](https://forms.office.com/pages/responsepage.aspx?id=lMFb0L-U1kquLh2w8uOPXhksOXzZ73RCp9fVTz4vTU5UNTc1U00yNVUxWVg4SkJGMFVHN1RCTTdQRS4u) to the FSDH Team

- Uploading, downloading, and deleting with the Storage Explorer

  > The Storage Explorer is a tool that allows you to upload and download files to and from the FSDH. You can find more information about the Storage Explorer in the [documentation](https://federal-science-datahub.canada.ca/resources/113d3041-e2a6-4043-8f83-2d2bdb0639c5)

- Using `azcopy` to sync files between your local machine and the FSDH

  > `azcopy` is a tool that allows you to sync files between your local machine and the FSDH Storage. You can find more information about `azcopy` in the [documentation](https://federal-science-datahub.canada.ca/resources/5a6b42aa-d48a-4e04-8383-751699273d7e)

- Mounting files from the FSDH Storage Explorer to your Databricks notebook

  > You can mount files from the FSDH directly into your Databricks notebook. You can find more information about mounting files in the [documentation](https://federal-science-datahub.canada.ca/resources/24e0b610-5d4d-46b2-99df-73e847abd630)

- Overview of notebook cells (python, markdown, R) and how to run them on a Databricks cluster
  > You can find more information about markdown and how to format it on the following [website](https://www.markdownguide.org/basic-syntax/)

<br>

---

## Up Next

The next office hour will be on April 5th, 2023 at 13:00 EST. Where we will cover the following topics:

- [Using spark to work with data files](https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/05-write-spark-code)
